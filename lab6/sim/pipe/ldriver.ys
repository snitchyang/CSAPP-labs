#######################################################################
# Test for copying block of size 63;
#######################################################################
	.pos 0
main:	irmovq Stack, %rsp  	# Set up stack pointer

	# Set up arguments for copy function and then invoke it
	irmovq $63, %rdx		# src and dst have 63 elements
	irmovq dest, %rsi	# dst array
	irmovq src, %rdi	# src array
	call ncopy		 
	halt			# should halt with num nonzeros in %rax
StartFun:
# 杨征 521021910478
# I use iaddq, loop unrolling and binary search to optimize the code. 
# More specifically, I handle the array 8 at a time, then 4, then 2, then 1. 
# I tidied up the code so the original comments are gone
ncopy:
    iaddq $-1, %rdx
    jne Start
    mrmovq (%rdi), %r12	# 
	andq %r12, %r12		
	jle Count		
	irmovq $1, %rax		# count = 1		
Count:
	rmmovq %r12, (%rsi)	# ...and store it to dst
	ret

Start:
    iaddq $-7, %rdx
    # jge Eight
    jl Four_start
Eight:
    mrmovq (%rdi), %r8
    mrmovq 8(%rdi), %r9
    mrmovq 16(%rdi), %r10
    mrmovq 24(%rdi), %r11
    mrmovq 32(%rdi), %r12
    mrmovq 40(%rdi), %r13
    mrmovq 48(%rdi), %r14
    mrmovq 56(%rdi), %rcx
    # update count , count ++ if val > 0
    andq %r8, %r8
    jle Eight_one
    iaddq $1, %rax
Eight_one:
    andq %r9, %r9
    jle Eight_two
    iaddq $1, %rax
Eight_two:
    andq %r10, %r10
    jle Eight_three
    iaddq $1, %rax
Eight_three:
    andq %r11, %r11
    jle Eight_four
    iaddq $1, %rax
Eight_four:
    andq %r12, %r12
    jle Eight_five
    iaddq $1, %rax
Eight_five:
    andq %r13, %r13
    jle Eight_six
    iaddq $1, %rax
Eight_six:
    andq %r14, %r14
    jle Eight_seven
    iaddq $1, %rax
Eight_seven:
    andq %rcx, %rcx
    jle Eight_eight
    iaddq $1, %rax
Eight_eight:
    rmmovq %r8, (%rsi)
    rmmovq %r9, 8(%rsi)
    rmmovq %r10, 16(%rsi)
    rmmovq %r11, 24(%rsi)
    rmmovq %r12, 32(%rsi)
    rmmovq %r13, 40(%rsi)
    rmmovq %r14, 48(%rsi)
    rmmovq %rcx, 56(%rsi)
    iaddq $64, %rdi
    iaddq $64, %rsi
    iaddq $-8, %rdx 
    jge Eight

Four_start:
    iaddq $4, %rdx
    jl Two
Four:
    mrmovq (%rdi), %r8
    mrmovq 8(%rdi), %r9
    mrmovq 16(%rdi), %r10
    mrmovq 24(%rdi), %r11
    # update count , count ++ if val > 0
    andq %r8, %r8
    jle Four_one
    iaddq $1, %rax
Four_one:
    andq %r9, %r9
    jle Four_two
    iaddq $1, %rax
Four_two:
    andq %r10, %r10
    jle Four_three
    iaddq $1, %rax
Four_three:
    andq %r11, %r11
    jle Four_four
    iaddq $1, %rax
Four_four:
    rmmovq %r8, (%rsi)
    rmmovq %r9, 8(%rsi)
    rmmovq %r10, 16(%rsi)
    rmmovq %r11, 24(%rsi)
    iaddq $32, %rdi
    iaddq $32, %rsi
    iaddq $-4, %rdx # len -= 4
    
Two:
    iaddq $2, %rdx
    jl One
    mrmovq (%rdi), %r8
    mrmovq 8(%rdi), %r9
    # update count , count ++ if val > 0
    andq %r8, %r8
    jle Two_one
    iaddq $1, %rax
Two_one:
    andq %r9, %r9
    jle Two_two
    iaddq $1, %rax
Two_two:
    rmmovq %r8, (%rsi)
    rmmovq %r9, 8(%rsi)
    iaddq $16, %rdi
    iaddq $16, %rsi
    iaddq $-2, %rdx # len -= 1

One:
    iaddq $2, %rdx
    je Done
    mrmovq (%rdi), %r8
    # update count , count ++ if val > 0
    andq %r8, %r8
    jle One_one
    iaddq $1, %rax
One_one:
    rmmovq %r8, (%rsi)
    # iaddq $8, %rdi
    # iaddq $8, %rsi
    # iaddq $-1, %rdx # len -= 1
Done:
    ret

EndFun:

###############################
# Source and destination blocks 
###############################
	.align 8
src:
	.quad 1
	.quad 2
	.quad 3
	.quad 4
	.quad 5
	.quad 6
	.quad -7
	.quad -8
	.quad 9
	.quad -10
	.quad -11
	.quad -12
	.quad -13
	.quad 14
	.quad -15
	.quad 16
	.quad -17
	.quad -18
	.quad -19
	.quad 20
	.quad -21
	.quad 22
	.quad -23
	.quad -24
	.quad -25
	.quad -26
	.quad 27
	.quad 28
	.quad -29
	.quad 30
	.quad -31
	.quad -32
	.quad -33
	.quad -34
	.quad -35
	.quad 36
	.quad -37
	.quad 38
	.quad -39
	.quad 40
	.quad -41
	.quad -42
	.quad -43
	.quad 44
	.quad 45
	.quad 46
	.quad -47
	.quad -48
	.quad -49
	.quad 50
	.quad -51
	.quad 52
	.quad 53
	.quad 54
	.quad -55
	.quad -56
	.quad 57
	.quad 58
	.quad 59
	.quad 60
	.quad 61
	.quad 62
	.quad 63
	.quad 0xbcdefa # This shouldn't get moved

	.align 16
Predest:
	.quad 0xbcdefa
dest:
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
Postdest:
	.quad 0xdefabc

.align 8
# Run time stack
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0

Stack:
